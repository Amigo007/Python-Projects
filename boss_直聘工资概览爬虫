url='https://www.zhipin.com/c101210100/?query=人事主管&page='
#bosszhipin网址
headers={
  'user-agent':'Mozilla/5.0'
}
page=1
hud=['职位名','网址','薪资1','薪资2','职位名','地点','经验','学历','公司行业','融资阶段','公司人数','发布日期','发布人']
#print('\t'.join(hud))

import pandas as pd
from pandas import DataFrame
import requests
from bs4 import BeautifulSoup
import time


for n in range(1,11):  
    html=requests.get(url+str(page),headers=headers)
    print(html.status_code)
    page+=1  
    soup = BeautifulSoup(html.text, 'html.parser')
    for item in soup.find_all('div','job-primary'):
      shuchu=[]
      shuchu.append(item.find('div','job-title').string) #职位名

      xinzi=item.find('span','red').string
      xinzi=xinzi.replace('k','')
      xinzi=xinzi.split('-')
      shuchu.append(xinzi[0]) #薪资起始数
      shuchu.append(xinzi[1]) #薪资起始数

      yaoqiu=item.find('p').contents
      shuchu.append(yaoqiu[0].string  if len(yaoqiu)>0 else 'None') #地点
      shuchu.append(yaoqiu[2].string  if len(yaoqiu)>2 else 'None') #经验
      shuchu.append(yaoqiu[4].string  if len(yaoqiu)>4 else 'None') #学历

      gongsi=item.find('div','info-company').find('p').contents
      shuchu.append(gongsi[0].string  if len(gongsi)>0 else 'None') #公司行业
      shuchu.append(gongsi[2].string  if len(gongsi)>2 else 'None') #融资阶段
      shuchu.append(gongsi[4].string if len(gongsi)>4 else 'None') #公司人数

      shuchu.append(item.find('div','info-publis').find('p').string.replace('发布于','')) #发布日期
      shuchu.append(item.find('div','info-publis').find('h3').contents[3].string) #发布人

      
     
    
      wangzhi=item.find('a').get('href').replace('/job_detail/','https://www.zhipin.com/job_detail/')
      shuchu.append(wangzhi)  #网址超链接
    
      spc='\t'.join(shuchu)
      print(spc)
      
      
      c={'spc':spc}
      data=DataFrame(c,index=c)
      
      data.to_csv('d:/test/renshizhuguan.csv',mode='a')
      #存储到 本地csv文件，增加模式 不覆盖原有数据
      可以算出岗位分布 薪资 等相关数据


Python爬虫(02): 伪装浏览器爬取Boss直聘网页数据 - 辉小宝同学的文章 - 知乎
https://zhuanlan.zhihu.com/p/52572851




#20190108 更新加入 职位详情遍历 需优化
KW='数据'#input what you want to search
url='https://www.zhipin.com/c101210100/?query='+KW+ '&page='
headers={
  'user-agent':'Mozilla/5.0'
}
page=1
hud=['职位名','网址','薪资1','薪资2','职位名','地点','经验','学历','公司行业','融资阶段','公司人数','发布日期','发布人']
#print('\t'.join(hud))

import pandas as pd
from pandas import DataFrame
import requests
from bs4 import BeautifulSoup
import time


for n in range(1,21):  
    html=requests.get(url+str(page),headers=headers)
    print(html.status_code)
    page+=1  
    soup = BeautifulSoup(html.text, 'html.parser')
    for item in soup.find_all('div','job-primary'):
      shuchu=[]
      shuchu.append(item.find('div','job-title').string) #职位名

      xinzi=item.find('span','red').string
      xinzi=xinzi.replace('k','')
      xinzi=xinzi.split('-')
      shuchu.append(xinzi[0]) #薪资起始数
      shuchu.append(xinzi[1]) #薪资起始数

      yaoqiu=item.find('p').contents
      shuchu.append(yaoqiu[0].string  if len(yaoqiu)>0 else 'None') #地点
      shuchu.append(yaoqiu[2].string  if len(yaoqiu)>2 else 'None') #经验
      shuchu.append(yaoqiu[4].string  if len(yaoqiu)>4 else 'None') #学历

      gongsi=item.find('div','info-company').find('p').contents
      shuchu.append(gongsi[0].string  if len(gongsi)>0 else 'None') #公司行业
      shuchu.append(gongsi[2].string  if len(gongsi)>2 else 'None') #融资阶段
      shuchu.append(gongsi[4].string if len(gongsi)>4 else 'None') #公司人数

      shuchu.append(item.find('div','info-publis').find('p').string.replace('发布于','')) #发布日期
      shuchu.append(item.find('div','info-publis').find('h3').contents[3].string) #发布人

      
     
    
      wangzhi=item.find('a').get('href').replace('/job_detail/','https://www.zhipin.com/job_detail/')
      ''' 
      遍历职位详情
          for w in enumerate(wangzhi):
            ht=requests.get(wz1,headers =headers)
            soup = BeautifulSoup(ht.text, 'html.parser')
            item = soup.find(attrs={'class':'text'}).text
            print(item)
     '''
      shuchu.append(wangzhi)#wangzhi
  
      spc='\t'.join(shuchu)
      print(spc)
      
      
      c={'spc':spc}
      data=DataFrame(c,index=c)
      
      data.to_csv('d:/test/'+KW+'.csv',mode='a')
      
 
####20190108更改 遍历网址问题修复，将网址加上{wangzhi}  列表化， 则可爬取所需要的信息
KW='PMP'#input what you want to search
url='https://www.zhipin.com/c101210100/?query='+KW+ '&page='
headers={
  'user-agent':'Mozilla/5.0'
}
page=1
hud=['职位名','网址','薪资1','薪资2','职位名','地点','经验','学历','公司行业','融资阶段','公司人数','发布日期','发布人']
#print('\t'.join(hud))

import pandas as pd
from pandas import DataFrame
import requests
from bs4 import BeautifulSoup
import time


for n in range(1,21):  
    html=requests.get(url+str(page),headers=headers)
    #print(html.status_code)
    page+=1  
    soup = BeautifulSoup(html.text, 'html.parser')
    for item in soup.find_all('div','job-primary'):
      shuchu=[]
      shuchu.append(item.find('div','job-title').string) #职位名

      xinzi=item.find('span','red').string
      xinzi=xinzi.replace('k','')
      xinzi=xinzi.split('-')
      shuchu.append(xinzi[0]) #薪资起始数
      shuchu.append(xinzi[1]) #薪资起始数

      yaoqiu=item.find('p').contents
      shuchu.append(yaoqiu[0].string  if len(yaoqiu)>0 else 'None') #地点
      shuchu.append(yaoqiu[2].string  if len(yaoqiu)>2 else 'None') #经验
      shuchu.append(yaoqiu[4].string  if len(yaoqiu)>4 else 'None') #学历

      gongsi=item.find('div','info-company').find('p').contents
      shuchu.append(gongsi[0].string  if len(gongsi)>0 else 'None') #公司行业
      shuchu.append(gongsi[2].string  if len(gongsi)>2 else 'None') #融资阶段
      shuchu.append(gongsi[4].string if len(gongsi)>4 else 'None') #公司人数

      shuchu.append(item.find('div','info-publis').find('p').string.replace('发布于','')) #发布日期
      shuchu.append(item.find('div','info-publis').find('h3').contents[3].string) #发布人


      wangzhi=item.find('a').get('href').replace('/job_detail/','https://www.zhipin.com/job_detail/')
      #shuchu.append(wangzhi)#wangzhi

      #遍历职位详情
      for uu in {wangzhi}:
        ht = requests.get(uu,headers = headers)
        soup = BeautifulSoup(ht.text, 'html.parser')
        item = soup.find(attrs={'class':'text'}).text
        shuchu.append(item)#wangzhi
      print(shuchu)
    
      spc='\t'.join(shuchu)
      print(spc)
      
   
      c={'spc':spc}
      data=DataFrame(c,index=c)
      
      data.to_csv('d:/test/'+KW+1+'.csv',mode='a')
    

     
      
